{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11996954624"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10638852096"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507496960"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id = os.listdir( './data/train')\n",
    "# image_id_set = []\n",
    "# for i in image_id:\n",
    "#     i = i.split('.')[0]\n",
    "#     image_id_set.append(i)\n",
    "# image_id_set = set(image_id_set)\n",
    "#image_id = set(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147793, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height                         bbox   source\n",
       "0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n",
       "1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n",
       "2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n",
       "3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n",
       "4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "train_df.drop(columns=['bbox'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['x'] = train_df['x'].astype(np.float)\n",
    "train_df['y'] = train_df['y'].astype(np.float)\n",
    "train_df['w'] = train_df['w'].astype(np.float)\n",
    "train_df['h'] = train_df['h'].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize bbox 512/1024\n",
    "train_df['x'] = train_df['x']*(250/1024)\n",
    "train_df['y'] = train_df['y']*(250/1024)\n",
    "train_df['w'] = train_df['w']*(250/1024)\n",
    "train_df['h'] = train_df['h']*(250/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>203.613281</td>\n",
       "      <td>54.199219</td>\n",
       "      <td>13.671875</td>\n",
       "      <td>8.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>55.175781</td>\n",
       "      <td>133.789062</td>\n",
       "      <td>31.738281</td>\n",
       "      <td>14.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>92.041016</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>18.066406</td>\n",
       "      <td>39.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>203.613281</td>\n",
       "      <td>23.193359</td>\n",
       "      <td>26.611328</td>\n",
       "      <td>26.123047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>6.347656</td>\n",
       "      <td>35.156250</td>\n",
       "      <td>30.273438</td>\n",
       "      <td>28.564453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source           x           y          w  \\\n",
       "0  b6ab77fd7   1024    1024  usask_1  203.613281   54.199219  13.671875   \n",
       "1  b6ab77fd7   1024    1024  usask_1   55.175781  133.789062  31.738281   \n",
       "2  b6ab77fd7   1024    1024  usask_1   92.041016  123.046875  18.066406   \n",
       "3  b6ab77fd7   1024    1024  usask_1  203.613281   23.193359  26.611328   \n",
       "4  b6ab77fd7   1024    1024  usask_1    6.347656   35.156250  30.273438   \n",
       "\n",
       "           h  \n",
       "0   8.789062  \n",
       "1  14.160156  \n",
       "2  39.062500  \n",
       "3  26.123047  \n",
       "4  28.564453  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split 0.8/0.2\n",
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25006, 8), (122787, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        #print(image_id)\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        #reszie image to 512*512\n",
    "        dim = (250, 250)\n",
    "        resized_img = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "        resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        resized_img /= 255.0 #normalization\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = torch.tensor(boxes)\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': resized_img,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2,\n",
    "                                      sat_shift_limit=0.2,\n",
    "                                      val_shift_limit=0.2,\n",
    "                                      p=0.5),\n",
    "                 A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                            contrast_limit=0.2,\n",
    "                                            p=0.5)],\n",
    "                p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model; pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # 1 class (wheat) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets, image_ids = tuple(zip(*batch))\n",
    "    images = torch.stack(images)\n",
    "    return images, targets, image_ids\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = WheatDataset(train_df, '../data/train', get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, '../data/train', get_valid_transform())\n",
    "# train_dataset = WheatDataset(train_df, './data/train')\n",
    "# valid_dataset = WheatDataset(valid_df, './data/train')\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images = images.permute(0, 2, 3, 1) #[torch.as_tensor(image, dtype = torch.float32).permute(1,2,0).cuda() for image in images]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 250, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 2\n",
    "boxes = targets[sample_id]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[sample_id].permute(0,1,2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-da37db6062c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m220\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type tuple)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(220,0,0),2)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, targets, image_ids = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=3,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=3,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.7150919437408447\n",
      "Iteration #100 loss: 0.7599583268165588\n",
      "Iteration #150 loss: 0.8132670521736145\n",
      "Iteration #200 loss: 0.5722005367279053\n",
      "Iteration #250 loss: 0.6252717971801758\n",
      "Iteration #300 loss: 0.5724431276321411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8b1412b40a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: <ExecutionResult object at 7f745927ba90, execution_count=40 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7f745927b438, raw_cell=\"loss_hist = Averager()\nitr = 1\nfor epoch in range(..\" store_history=True silent=False shell_futures=True> result=None>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in train_data_loader:        \n",
    "            images = images.to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            \n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            \n",
    "            loss_hist.send(loss_value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "                        \n",
    "            itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(img.to(device) for img in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[1].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
